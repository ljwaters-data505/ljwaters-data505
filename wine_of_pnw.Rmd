---
title: "Wine of the PNW"
author: "Landon Waters"
date: "2025-01-15"
output: html_document
---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](src/wine_of_pnw.qmd) hosted on GitHub pages.

# Setup

**Step Up Code:**
```{r echo=FALSE, message=FALSE}
library(tidyverse)
```

```{r}
wine <- readRDS(gzcon(url("https://github.com/cd-public/DSLM-505/raw/master/dat/wine.rds"))) %>%
  filter(province=="Oregon" | province=="California" | province=="New York") %>% 
  mutate(cherry=as.integer(str_detect(description,"[Cc]herry"))) %>% 
  mutate(lprice=log(price)) %>% 
  select(lprice, points, cherry, province)
```

**Explanataion:**

> <span style="color:red;font-weight:bold">TODO</span>: *write your line-by-line explanation of the code here* 

The function gzcon reads in the compressed data set from the github URL, and readRDS reads the data object in R, assinging it to "wine". Next, the filter is making sure only Oregon, California, and New York provinces are included. The first mutate makes a new column called cherry that looks for the either "cherry" or "Cherry" from the description column with str_detect (creating a boolean T/F), and makes it an integer value (1/0) with as.integer. The next mutate makes a new column called lprice whose values are equal to the log of the price column. Finally, select takes the columns lprice, points, cherry, and province from the data set, and assigns it to a new data set called "wine".

# Multiple Regression

## Linear Models

First run a linear regression model with log of price as the dependent variable and 'points' and 'cherry' as features (variables).

```{r}
# TODO: hint: m1 <- lm(lprice ~ points + cherry)
library(moderndive)

m1 <- lm(lprice ~ points + cherry, data = wine)
get_regression_points(m1)%>%
  drop_na(residual) %>%
  mutate(sq_residuals = residual^2) %>%
  summarise(rmse = sqrt(mean(sq_residuals))) %>%
  pluck("rmse")

sd(wine$lprice) # = 0.5625001
```

**Explanataion:**

> <span style="color:red;font-weight:bold">TODO</span>: *write your line-by-line explanation of the code here* 

Creates a linear regression model where the dependent variable is lprice, and the features are points and cherry. The function get_regression_points(m1) produces a tibble of actual values, predicted values, and the residuals. We take out any NA values in the residual column using drop_na() and create a new column for squared residuals with mutate(). A new value rmse takes the root mean of the sq_residuals variable, and summarise() creates a tibble containing the value of rmse. The pluck function returns rmse from the tibble.

> <span style="color:red;font-weight:bold">TODO</span>: *report and explain the RMSE* 

RMSE = 0.4687604: This is not too bad given the values we are working with, and it is lower than the standard deviation of the dependent variable (0.5625001)

## Interaction Models

Add an interaction between 'points' and 'cherry'. 

```{r}
# TODO: hint: Check the slides.
m2 <- lm(lprice ~ points * cherry, data = wine)
get_regression_points(m2)%>%
  drop_na(residual) %>%
  mutate(sq_residuals = residual^2) %>%
  summarise(rmse = sqrt(mean(sq_residuals))) %>%
  pluck("rmse")
```

> <span style="color:red;font-weight:bold">TODO</span>: *write your line-by-line explanation of the code here* 

The only change from the previous question is adding the interaction between points and cherry. The interaction captures how the effect of one variable changes depending on the level of the other variable.

> <span style="color:red;font-weight:bold">TODO</span>: *report and explain the RMSE* 

RMSE = 0.4685214: This is a very slight improvement on the previous model, but not by much. You could argue that the interaction does not have a very significant impact in the models prediction.

### The Interaction Variable

> <span style="color:red;font-weight:bold">TODO</span>: *interpret the coefficient on the interaction variable.* <br>[Explain as you would to a non-technical manager.](https://youtube.com/clip/UgkxY7ohjoimIef6zpPLjgQHqJcJHeZptuVm?feature=shared) 

The coefficient for the interaction variable captures the effect that points has on lprice whenever cherry is TRUE. The coeffecient is 0.01266, which is basically saying that whenever the wine has a cherry flavor, you can expect points to have a slightly stronger impact on increasing price than if it did not.

## Applications

Determine which province (Oregon, California, or New York), does the 'cherry' feature in the data affect price most?

```{r}
# TODO: 
lm_or <- lm(lprice ~ points + cherry, data = filter(wine, province == "Oregon"))
lm_ca <- lm(lprice ~ points + cherry, data = filter(wine, province == "California"))
lm_ny <- lm(lprice ~ points + cherry, data = filter(wine, province == "New York"))

lm_or
lm_ca
lm_ny
```

> <span style="color:red;font-weight:bold">TODO</span>: *write your line-by-line explanation of the code here, and explain your answer.* 

Here I am just making models like we did before with cherry as a feature, but this time I filter the wine data set to each specific province. Then I just print the coefficients for each model to compare. All three models have positive coefficients for cherry, but Oregon has the largest coefficient. So, holding all else constant, in Oregon the cherry feature seems to positively affect lprice the most.

# Scenarios

## On Accuracy

Imagine a model to distinguish New York wines from those in California and Oregon. After a few days of work, you take some measurements and note: "I've achieved 91% accuracy on my model!" 

Should you be impressed? Why or why not?

```{r}
# TODO: Use simple descriptive statistics from the data to justify your answer.
table(wine$province)%>%
  pluck('New York')/length(wine$province)
```

> <span style="color:red;font-weight:bold">TODO</span>: *describe your reasoning here* 

On the surface this looks really good, but in reality it is not very impressive. The code above shows the distribution of New York wines in the data set (8.9%). This means that if the model simply guesses Oregon/California every single time, they would get the same accuracy. 

## On Ethics

Why is understanding this vignette important to use machine learning in an ethical manner?

> <span style="color:red;font-weight:bold">TODO</span>: *describe your reasoning here* 

In this scenario, a minority group was assumed to be described by a set of variables despite it being completely inaccurate which could lead to misclassifications of certain groups. As models become more complex and opaque, this issue becomes more pressing as assumptions are enshrouded.

## Ignorance is no excuse
Imagine you are working on a model to predict the likelihood that an individual loses their job as the result of the changing federal policy under new presidential administrations. You have a very large dataset with many hundreds of features, but you are worried that including indicators like age, income or gender might pose some ethical problems. When you discuss these concerns with your boss, she tells you to simply drop those features from the model. Does this solve the ethical issue? Why or why not?

> <span style="color:red;font-weight:bold">TODO</span>: *describe your reasoning here* 

No, by excluding them the model will now just ignore the disparities that the data may be showing and obscure insights into larger underlying issues. I believe that as long as you are aware and understand what implications (especially in how they can be misinterpretted to perpetuate ongoing steretypes and bias) including the varaibles may have, it is better to include than exclude.
